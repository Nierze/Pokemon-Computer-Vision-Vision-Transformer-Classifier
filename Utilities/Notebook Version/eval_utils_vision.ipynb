{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOUSdRJyHjMJt1aAIe0wxfn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Computer Vision Evaluation Utilities Module 1.0.0\n","#### Made by: Melchor Filippe S. Bulanon\n","#### Last Updated: 02/02/2025\n","\n","This module contains all the functions necessary to evaluate computer vision models created in pytorch."],"metadata":{"id":"1TpcDNH_pk7U"}},{"cell_type":"markdown","source":["## Import necessary modules"],"metadata":{"id":"gPI3BwB6pyXE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NL65R-J_pjmO"},"outputs":[],"source":["from typing import Optional, Dict, List, Tuple\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchmetrics import Accuracy, Precision, Recall, F1Score, ConfusionMatrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import confusion_matrix, classification_report\n"]},{"cell_type":"markdown","source":["## Evaluate Model Function"],"metadata":{"id":"4ElUrQLnsB7Z"}},{"cell_type":"code","source":["def evaluate_model(\n","    model: nn.Module,\n","    dataloader: DataLoader,\n","    device: str = 'cuda',\n","    num_classes: int = 10,\n","    confidence_threshold: float = 0.5,\n","    return_predictions: bool = False\n",") -> Dict:\n","    \"\"\"\n","    Evaluate a PyTorch computer vision model with multiple metrics.\n","\n","    Args:\n","        model: PyTorch model to evaluate\n","        dataloader: DataLoader containing validation/test data\n","        device: Device to run evaluation on ('cuda' or 'cpu')\n","        num_classes: Number of classes in the classification task\n","        confidence_threshold: Threshold for confidence scores\n","        return_predictions: Whether to return model predictions\n","\n","    Returns:\n","        Dictionary containing evaluation metrics\n","    \"\"\"\n","    model.eval()\n","    model.to(device)\n","\n","    # Initialize metrics\n","    metrics = {\n","        'accuracy': Accuracy(task='multiclass', num_classes=num_classes).to(device),\n","        'precision': Precision(task='multiclass', num_classes=num_classes).to(device),\n","        'recall': Recall(task='multiclass', num_classes=num_classes).to(device),\n","        'f1': F1Score(task='multiclass', num_classes=num_classes).to(device),\n","        'confusion_matrix': ConfusionMatrix(task='multiclass', num_classes=num_classes).to(device)\n","    }\n","\n","    all_preds = []\n","    all_labels = []\n","    all_confidences = []\n","\n","    with torch.inference_mode():\n","        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n","            images, labels = batch\n","            images, labels = images.to(device), labels.to(device)\n","\n","            # Get model predictions\n","            outputs = model(images)\n","            probs = torch.softmax(outputs, dim=1)\n","            confidences, predictions = torch.max(probs, dim=1)\n","\n","            # Store predictions and labels\n","            all_preds.append(predictions)\n","            all_labels.append(labels)\n","            all_confidences.append(confidences)\n","\n","            # Update metrics\n","            for metric in metrics.values():\n","                metric.update(predictions, labels)\n","\n","    # Concatenate all predictions and labels\n","    all_preds = torch.cat(all_preds)\n","    all_labels = torch.cat(all_labels)\n","    all_confidences = torch.cat(all_confidences)\n","\n","    # Calculate metrics\n","    results = {\n","        'accuracy': metrics['accuracy'].compute().item(),\n","        'precision': metrics['precision'].compute().mean().item(),\n","        'recall': metrics['recall'].compute().mean().item(),\n","        'f1': metrics['f1'].compute().mean().item(),\n","        'confusion_matrix': metrics['confusion_matrix'].compute().cpu().numpy()\n","    }\n","\n","    # Calculate per-class metrics\n","    class_precision = metrics['precision'].compute()\n","    class_recall = metrics['recall'].compute()\n","    class_f1 = metrics['f1'].compute()\n","\n","    results['per_class_metrics'] = {\n","        f'class_{i}': {\n","            'precision': class_precision[i].item(),\n","            'recall': class_recall[i].item(),\n","            'f1': class_f1[i].item()\n","        } for i in range(num_classes)\n","    }\n","\n","    # Calculate confidence statistics\n","    results['confidence_stats'] = {\n","        'mean_confidence': all_confidences.mean().item(),\n","        'min_confidence': all_confidences.min().item(),\n","        'max_confidence': all_confidences.max().item()\n","    }\n","\n","    # Optionally return predictions\n","    if return_predictions:\n","        results['predictions'] = {\n","            'preds': all_preds.cpu(),\n","            'labels': all_labels.cpu(),\n","            'confidences': all_confidences.cpu()\n","        }\n","\n","    # Plot confusion matrix\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(\n","        results['confusion_matrix'],\n","        annot=True,\n","        fmt='d',\n","        cmap='Blues',\n","        xticklabels=[f'Class {i}' for i in range(num_classes)],\n","        yticklabels=[f'Class {i}' for i in range(num_classes)]\n","    )\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    results['confusion_matrix_plot'] = plt\n","\n","    return results"],"metadata":{"id":"AAI9BuYBpxuv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Print Evaluation Results Function"],"metadata":{"id":"LeYA-UUxsg62"}},{"cell_type":"code","source":["def print_evaluation_results(results: Dict) -> None:\n","    \"\"\"\n","    Pretty print the evaluation results.\n","\n","    Args:\n","        results: Dictionary containing evaluation metrics\n","    \"\"\"\n","    print(\"\\n=== Model Evaluation Results ===\")\n","    print(f\"\\nOverall Metrics:\")\n","    print(f\"Accuracy: {results['accuracy']:.4f}\")\n","    print(f\"Precision: {results['precision']:.4f}\")\n","    print(f\"Recall: {results['recall']:.4f}\")\n","    print(f\"F1 Score: {results['f1']:.4f}\")\n","\n","    print(\"\\nConfidence Statistics:\")\n","    print(f\"Mean Confidence: {results['confidence_stats']['mean_confidence']:.4f}\")\n","    print(f\"Min Confidence: {results['confidence_stats']['min_confidence']:.4f}\")\n","    print(f\"Max Confidence: {results['confidence_stats']['max_confidence']:.4f}\")\n","\n","    print(\"\\nPer-Class Metrics:\")\n","    for class_name, metrics in results['per_class_metrics'].items():\n","        print(f\"\\n{class_name}:\")\n","        print(f\"  Precision: {metrics['precision']:.4f}\")\n","        print(f\"  Recall: {metrics['recall']:.4f}\")\n","        print(f\"  F1: {metrics['f1']:.4f}\")"],"metadata":{"id":"eIZxgBUhpxxN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create Confusion Matrix"],"metadata":{"id":"1oA1Jhuqu198"}},{"cell_type":"code","source":["def analyze_confusion_matrix(\n","    model: nn.Module,\n","    dataloader: DataLoader,\n","    class_names: Optional[List[str]] = None,\n","    device: str = 'cuda',\n","    figsize: Tuple[int, int] = (12, 8),\n","    cmap: str = 'Blues'\n",") -> Dict:\n","    \"\"\"\n","    Generate and analyze confusion matrix with classification metrics.\n","\n","    Args:\n","        model: PyTorch model to evaluate\n","        dataloader: DataLoader containing validation/test data\n","        class_names: List of class names (optional)\n","        device: Device to run evaluation on ('cuda' or 'cpu')\n","        figsize: Figure size for the plots\n","        cmap: Color map for the confusion matrix\n","    \"\"\"\n","    model.eval()\n","    model.to(device)\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    # Collect predictions\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Computing predictions\"):\n","            images, labels = batch\n","            images, labels = images.to(device), labels.to(device)\n","\n","            outputs = model(images)\n","            _, predictions = torch.max(outputs, 1)\n","\n","            all_preds.extend(predictions.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Convert to numpy arrays\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","\n","    # Get number of classes\n","    n_classes = len(np.unique(all_labels))\n","\n","    # Use provided class names or generate defaults\n","    if class_names is None:\n","        class_names = [f'Class {i}' for i in range(n_classes)]\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(all_labels, all_preds)\n","    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    # Get classification report\n","    report = classification_report(all_labels, all_preds,\n","                                 target_names=class_names,\n","                                 output_dict=True)\n","\n","    # Create visualization\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n","\n","    # Raw counts\n","    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, ax=ax1,\n","                xticklabels=class_names, yticklabels=class_names)\n","    ax1.set_title('Confusion Matrix (Raw Counts)')\n","    ax1.set_xlabel('Predicted')\n","    ax1.set_ylabel('True')\n","\n","    # Normalized\n","    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap=cmap, ax=ax2,\n","                xticklabels=class_names, yticklabels=class_names)\n","    ax2.set_title('Confusion Matrix (Normalized)')\n","    ax2.set_xlabel('Predicted')\n","    ax2.set_ylabel('True')\n","\n","    plt.tight_layout()\n","\n","    # Find most confused pairs\n","    confused_pairs = []\n","    for i in range(n_classes):\n","        for j in range(n_classes):\n","            if i != j:\n","                confused_pairs.append({\n","                    'true_class': class_names[i],\n","                    'predicted_class': class_names[j],\n","                    'count': cm[i, j],\n","                    'percentage': cm_normalized[i, j]\n","                })\n","\n","    confused_pairs.sort(key=lambda x: x['count'], reverse=True)\n","\n","    return {\n","        'confusion_matrix': cm,\n","        'confusion_matrix_normalized': cm_normalized,\n","        'classification_report': report,\n","        'most_confused_pairs': confused_pairs[:5],\n","        'plot': fig\n","    }"],"metadata":{"id":"THNoqHEbpxzZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Print Confusion Matrix Analysis"],"metadata":{"id":"1li9Rmdhu-pw"}},{"cell_type":"code","source":["def print_confusion_analysis(results: Dict) -> None:\n","    \"\"\"\n","    Print analysis of confusion matrix results.\n","    \"\"\"\n","    print(\"\\n=== Model Performance Analysis ===\")\n","\n","    # Print overall metrics\n","    report = results['classification_report']\n","    print(f\"\\nOverall Metrics:\")\n","    print(f\"Accuracy: {report['accuracy']:.4f}\")\n","    print(f\"Macro Avg F1-Score: {report['macro avg']['f1-score']:.4f}\")\n","    print(f\"Weighted Avg F1-Score: {report['weighted avg']['f1-score']:.4f}\")\n","\n","    # Print per-class metrics\n","    print(\"\\nPer-Class Metrics:\")\n","    for class_name, metrics in report.items():\n","        if class_name not in ['accuracy', 'macro avg', 'weighted avg']:\n","            print(f\"\\n{class_name}:\")\n","            print(f\"  Precision: {metrics['precision']:.4f}\")\n","            print(f\"  Recall: {metrics['recall']:.4f}\")\n","            print(f\"  F1-Score: {metrics['f1-score']:.4f}\")\n","            print(f\"  Support: {metrics['support']}\")\n","\n","    # Print most confused pairs\n","    print(\"\\nTop 5 Most Confused Pairs:\")\n","    for pair in results['most_confused_pairs']:\n","        print(f\"\\nTrue: {pair['true_class']} â†’ Predicted: {pair['predicted_class']}\")\n","        print(f\"  Count: {pair['count']}\")\n","        print(f\"  Percentage: {pair['percentage']:.2%}\")"],"metadata":{"id":"ZX9MrPQYpx1y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Show Confusion Matrix"],"metadata":{"id":"6lZYJfWbwq1u"}},{"cell_type":"code","source":["def show_confusion_matrix(\n","    model: torch.nn.Module,\n","    dataloader: torch.utils.data.DataLoader,\n","    class_names: Optional[List[str]] = None,\n","    device: str = 'cuda',\n","    figsize: Tuple[int, int] = (15, 10),\n","    save_path: Optional[str] = None,\n","    show_plot: bool = True\n",") -> None:\n","    \"\"\"\n","    Analyze and visualize model performance using confusion matrix and classification metrics.\n","\n","    Args:\n","        model: PyTorch model to evaluate\n","        dataloader: DataLoader containing validation/test data\n","        class_names: List of class names\n","        device: Device to run evaluation on ('cuda' or 'cpu')\n","        figsize: Figure size for the plots\n","        save_path: Path to save the confusion matrix plot (optional)\n","        show_plot: Whether to display the plot\n","    \"\"\"\n","    # Get confusion matrix analysis\n","    results = analyze_confusion_matrix(\n","        model=model,\n","        dataloader=dataloader,\n","        class_names=class_names,\n","        device=device,\n","        figsize=figsize\n","    )\n","\n","    # Print detailed analysis\n","    print_confusion_analysis(results)\n","\n","    # Handle plot display and saving\n","    if save_path:\n","        results['plot'].savefig(save_path, bbox_inches='tight', dpi=300)\n","        print(f\"\\nConfusion matrix plot saved to: {save_path}\")\n","\n","    if show_plot:\n","        plt.show()\n","    else:\n","        plt.close()\n","\n","    return results\n","\n","# # Example usage with error handling\n","# def evaluate_model_performance(\n","#     model_path: str,\n","#     test_loader: torch.utils.data.DataLoader,\n","#     class_names: List[str],\n","#     save_dir: Optional[str] = None\n","# ) -> None:\n","#     \"\"\"\n","#     Wrapper function to evaluate model performance with error handling.\n","\n","#     Args:\n","#         model_path: Path to the saved model\n","#         test_loader: Test data loader\n","#         class_names: List of class names\n","#         save_dir: Directory to save results (optional)\n","#     \"\"\"\n","#     try:\n","#         # Load model\n","#         model = torch.load(model_path)\n","#         device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","#         # Create save path if directory is provided\n","#         save_path = f\"{save_dir}/confusion_matrix.png\" if save_dir else None\n","\n","#         # Run analysis\n","#         results = show_confusion_matrix(\n","#             model=model,\n","#             dataloader=test_loader,\n","#             class_names=class_names,\n","#             device=device,\n","#             save_path=save_path\n","#         )\n","\n","#         return results\n","\n","#     except Exception as e:\n","#         print(f\"Error during model evaluation: {str(e)}\")\n","#         return None"],"metadata":{"id":"APN0X19Bpx38"},"execution_count":null,"outputs":[]}]}